
==> Audit <==
|--------------|----------------|----------|--------|---------|---------------------|---------------------|
|   Command    |      Args      | Profile  |  User  | Version |     Start Time      |      End Time       |
|--------------|----------------|----------|--------|---------|---------------------|---------------------|
| start        |                | minikube | chucho | v1.33.1 | 01 Jul 24 21:45 -03 | 01 Jul 24 21:49 -03 |
| update-check |                | minikube | chucho | v1.33.1 | 01 Jul 24 23:45 -03 | 01 Jul 24 23:45 -03 |
| update-check |                | minikube | chucho | v1.33.1 | 02 Jul 24 09:23 -03 | 02 Jul 24 09:23 -03 |
| start        |                | minikube | chucho | v1.33.1 | 02 Jul 24 09:25 -03 | 02 Jul 24 09:25 -03 |
| service      | api-service    | minikube | chucho | v1.33.1 | 02 Jul 24 09:49 -03 |                     |
| service      | api-service    | minikube | chucho | v1.33.1 | 02 Jul 24 09:52 -03 | 02 Jul 24 09:57 -03 |
| update-check |                | minikube | chucho | v1.33.1 | 02 Jul 24 20:57 -03 | 02 Jul 24 20:57 -03 |
| update-check |                | minikube | chucho | v1.33.1 | 03 Jul 24 10:11 -03 | 03 Jul 24 10:11 -03 |
| start        |                | minikube | chucho | v1.33.1 | 03 Jul 24 11:31 -03 | 03 Jul 24 11:31 -03 |
| service      | api-service    | minikube | chucho | v1.33.1 | 03 Jul 24 11:45 -03 |                     |
| service      | api-service    | minikube | chucho | v1.33.1 | 03 Jul 24 11:46 -03 |                     |
| service      | api-service-v1 | minikube | chucho | v1.33.1 | 03 Jul 24 11:51 -03 |                     |
| service      | api-service-v1 | minikube | chucho | v1.33.1 | 03 Jul 24 11:59 -03 |                     |
|--------------|----------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/07/03 11:31:17
Running on machine: AR-i42672267
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0703 11:31:17.549712   35902 out.go:291] Setting OutFile to fd 1 ...
I0703 11:31:17.549851   35902 out.go:343] isatty.IsTerminal(1) = true
I0703 11:31:17.549854   35902 out.go:304] Setting ErrFile to fd 2...
I0703 11:31:17.549856   35902 out.go:343] isatty.IsTerminal(2) = true
I0703 11:31:17.549996   35902 root.go:338] Updating PATH: /home/chucho/.minikube/bin
W0703 11:31:17.550874   35902 root.go:314] Error reading config file at /home/chucho/.minikube/config/config.json: open /home/chucho/.minikube/config/config.json: no such file or directory
I0703 11:31:17.553456   35902 out.go:298] Setting JSON to false
I0703 11:31:17.556647   35902 start.go:129] hostinfo: {"hostname":"AR-i42672267","uptime":4813,"bootTime":1720012264,"procs":69,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"15e328df-8c4a-4e3d-af2c-d61f43763f17"}
I0703 11:31:17.556681   35902 start.go:139] virtualization:  guest
I0703 11:31:17.559236   35902 out.go:177] 😄  minikube v1.33.1 on Ubuntu 22.04 (amd64)
I0703 11:31:17.566551   35902 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0703 11:31:17.566650   35902 driver.go:392] Setting default libvirt URI to qemu:///system
I0703 11:31:17.566733   35902 notify.go:220] Checking for updates...
I0703 11:31:17.748180   35902 docker.go:122] docker version: linux-26.1.4:Docker Desktop
I0703 11:31:17.748284   35902 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0703 11:31:19.890454   35902 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.142149997s)
I0703 11:31:19.890839   35902 info.go:266] docker info: {ID:84ec396d-eb05-4439-87dd-04a83a9bc4a2 Containers:38 ContainersRunning:19 ContainersPaused:0 ContainersStopped:19 Images:61 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:183 OomKillDisable:true NGoroutines:178 SystemTime:2024-07-03 14:31:19.872269663 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:3 MemTotal:4111581184 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0703 11:31:19.890913   35902 docker.go:295] overlay module found
I0703 11:31:19.892838   35902 out.go:177] ✨  Using the docker driver based on existing profile
I0703 11:31:19.894371   35902 start.go:297] selected driver: docker
I0703 11:31:19.894375   35902 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/chucho:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0703 11:31:19.894439   35902 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0703 11:31:19.894510   35902 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0703 11:31:20.108034   35902 info.go:266] docker info: {ID:84ec396d-eb05-4439-87dd-04a83a9bc4a2 Containers:38 ContainersRunning:19 ContainersPaused:0 ContainersStopped:19 Images:61 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:183 OomKillDisable:true NGoroutines:178 SystemTime:2024-07-03 14:31:20.101442802 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:3 MemTotal:4111581184 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0703 11:31:20.108455   35902 cni.go:84] Creating CNI manager for ""
I0703 11:31:20.108465   35902 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0703 11:31:20.108508   35902 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/chucho:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0703 11:31:20.110634   35902 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0703 11:31:20.112437   35902 cache.go:121] Beginning downloading kic base image for docker with docker
I0703 11:31:20.114050   35902 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0703 11:31:20.115498   35902 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0703 11:31:20.115527   35902 preload.go:147] Found local preload: /home/chucho/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0703 11:31:20.115531   35902 cache.go:56] Caching tarball of preloaded images
I0703 11:31:20.115566   35902 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0703 11:31:20.116605   35902 preload.go:173] Found /home/chucho/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0703 11:31:20.116612   35902 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0703 11:31:20.116666   35902 profile.go:143] Saving config to /home/chucho/.minikube/profiles/minikube/config.json ...
I0703 11:31:20.137123   35902 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0703 11:31:20.137135   35902 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0703 11:31:20.137184   35902 cache.go:194] Successfully downloaded all kic artifacts
I0703 11:31:20.137215   35902 start.go:360] acquireMachinesLock for minikube: {Name:mk68b6abebe9d525ce0cd5f607c856bbd085a3ab Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0703 11:31:20.137306   35902 start.go:364] duration metric: took 70.122µs to acquireMachinesLock for "minikube"
I0703 11:31:20.137318   35902 start.go:96] Skipping create...Using existing machine configuration
I0703 11:31:20.137335   35902 fix.go:54] fixHost starting: 
I0703 11:31:20.137519   35902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0703 11:31:20.162398   35902 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0703 11:31:20.162421   35902 fix.go:138] unexpected machine state, will restart: <nil>
I0703 11:31:20.164187   35902 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0703 11:31:20.166069   35902 cli_runner.go:164] Run: docker start minikube
I0703 11:31:20.495418   35902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0703 11:31:20.512245   35902 kic.go:430] container "minikube" state is running.
I0703 11:31:20.513630   35902 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0703 11:31:20.533040   35902 profile.go:143] Saving config to /home/chucho/.minikube/profiles/minikube/config.json ...
I0703 11:31:20.533188   35902 machine.go:94] provisionDockerMachine start ...
I0703 11:31:20.533280   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:20.559291   35902 main.go:141] libmachine: Using SSH client type: native
I0703 11:31:20.562170   35902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 51074 <nil> <nil>}
I0703 11:31:20.562178   35902 main.go:141] libmachine: About to run SSH command:
hostname
I0703 11:31:20.572230   35902 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0703 11:31:23.723822   35902 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0703 11:31:23.723859   35902 ubuntu.go:169] provisioning hostname "minikube"
I0703 11:31:23.723932   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:23.742481   35902 main.go:141] libmachine: Using SSH client type: native
I0703 11:31:23.742633   35902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 51074 <nil> <nil>}
I0703 11:31:23.742639   35902 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0703 11:31:23.896002   35902 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0703 11:31:23.896077   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:23.912308   35902 main.go:141] libmachine: Using SSH client type: native
I0703 11:31:23.912433   35902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 51074 <nil> <nil>}
I0703 11:31:23.912445   35902 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0703 11:31:24.031273   35902 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0703 11:31:24.031582   35902 ubuntu.go:175] set auth options {CertDir:/home/chucho/.minikube CaCertPath:/home/chucho/.minikube/certs/ca.pem CaPrivateKeyPath:/home/chucho/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/chucho/.minikube/machines/server.pem ServerKeyPath:/home/chucho/.minikube/machines/server-key.pem ClientKeyPath:/home/chucho/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/chucho/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/chucho/.minikube}
I0703 11:31:24.031597   35902 ubuntu.go:177] setting up certificates
I0703 11:31:24.031607   35902 provision.go:84] configureAuth start
I0703 11:31:24.031666   35902 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0703 11:31:24.053271   35902 provision.go:143] copyHostCerts
I0703 11:31:24.054092   35902 exec_runner.go:144] found /home/chucho/.minikube/ca.pem, removing ...
I0703 11:31:24.054106   35902 exec_runner.go:203] rm: /home/chucho/.minikube/ca.pem
I0703 11:31:24.054151   35902 exec_runner.go:151] cp: /home/chucho/.minikube/certs/ca.pem --> /home/chucho/.minikube/ca.pem (1078 bytes)
I0703 11:31:24.054556   35902 exec_runner.go:144] found /home/chucho/.minikube/cert.pem, removing ...
I0703 11:31:24.054564   35902 exec_runner.go:203] rm: /home/chucho/.minikube/cert.pem
I0703 11:31:24.054592   35902 exec_runner.go:151] cp: /home/chucho/.minikube/certs/cert.pem --> /home/chucho/.minikube/cert.pem (1123 bytes)
I0703 11:31:24.056964   35902 exec_runner.go:144] found /home/chucho/.minikube/key.pem, removing ...
I0703 11:31:24.056973   35902 exec_runner.go:203] rm: /home/chucho/.minikube/key.pem
I0703 11:31:24.057004   35902 exec_runner.go:151] cp: /home/chucho/.minikube/certs/key.pem --> /home/chucho/.minikube/key.pem (1675 bytes)
I0703 11:31:24.058628   35902 provision.go:117] generating server cert: /home/chucho/.minikube/machines/server.pem ca-key=/home/chucho/.minikube/certs/ca.pem private-key=/home/chucho/.minikube/certs/ca-key.pem org=chucho.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0703 11:31:24.143335   35902 provision.go:177] copyRemoteCerts
I0703 11:31:24.145065   35902 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0703 11:31:24.145125   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:24.163727   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:24.254140   35902 ssh_runner.go:362] scp /home/chucho/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0703 11:31:24.271545   35902 ssh_runner.go:362] scp /home/chucho/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0703 11:31:24.285912   35902 ssh_runner.go:362] scp /home/chucho/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0703 11:31:24.299949   35902 provision.go:87] duration metric: took 268.331376ms to configureAuth
I0703 11:31:24.299962   35902 ubuntu.go:193] setting minikube options for container-runtime
I0703 11:31:24.300081   35902 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0703 11:31:24.300138   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:24.316364   35902 main.go:141] libmachine: Using SSH client type: native
I0703 11:31:24.316497   35902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 51074 <nil> <nil>}
I0703 11:31:24.316503   35902 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0703 11:31:24.452867   35902 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0703 11:31:24.452908   35902 ubuntu.go:71] root file system type: overlay
I0703 11:31:24.452979   35902 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0703 11:31:24.453037   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:24.467969   35902 main.go:141] libmachine: Using SSH client type: native
I0703 11:31:24.468086   35902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 51074 <nil> <nil>}
I0703 11:31:24.468123   35902 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0703 11:31:24.609754   35902 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0703 11:31:24.609823   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:24.625169   35902 main.go:141] libmachine: Using SSH client type: native
I0703 11:31:24.625295   35902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 51074 <nil> <nil>}
I0703 11:31:24.625303   35902 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0703 11:31:24.756246   35902 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0703 11:31:24.756260   35902 machine.go:97] duration metric: took 4.22306535s to provisionDockerMachine
I0703 11:31:24.756268   35902 start.go:293] postStartSetup for "minikube" (driver="docker")
I0703 11:31:24.756276   35902 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0703 11:31:24.756320   35902 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0703 11:31:24.756357   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:24.772086   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:24.865763   35902 ssh_runner.go:195] Run: cat /etc/os-release
I0703 11:31:24.868682   35902 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0703 11:31:24.868711   35902 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0703 11:31:24.868717   35902 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0703 11:31:24.868723   35902 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0703 11:31:24.868756   35902 filesync.go:126] Scanning /home/chucho/.minikube/addons for local assets ...
I0703 11:31:24.869095   35902 filesync.go:126] Scanning /home/chucho/.minikube/files for local assets ...
I0703 11:31:24.869384   35902 start.go:296] duration metric: took 113.109169ms for postStartSetup
I0703 11:31:24.869437   35902 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0703 11:31:24.869473   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:24.895037   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:24.981939   35902 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0703 11:31:24.984723   35902 fix.go:56] duration metric: took 4.847385212s for fixHost
I0703 11:31:24.984734   35902 start.go:83] releasing machines lock for "minikube", held for 4.847421473s
I0703 11:31:24.984778   35902 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0703 11:31:25.000864   35902 ssh_runner.go:195] Run: cat /version.json
I0703 11:31:25.000904   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:25.002557   35902 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0703 11:31:25.002608   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:25.020445   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:25.027945   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:25.367850   35902 ssh_runner.go:195] Run: systemctl --version
I0703 11:31:25.376372   35902 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0703 11:31:25.379925   35902 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0703 11:31:25.394599   35902 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0703 11:31:25.394656   35902 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0703 11:31:25.400022   35902 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0703 11:31:25.400033   35902 start.go:494] detecting cgroup driver to use...
I0703 11:31:25.400055   35902 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0703 11:31:25.400899   35902 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0703 11:31:25.410542   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0703 11:31:25.416980   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0703 11:31:25.423598   35902 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0703 11:31:25.423650   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0703 11:31:25.430195   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0703 11:31:25.436044   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0703 11:31:25.441705   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0703 11:31:25.447332   35902 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0703 11:31:25.453623   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0703 11:31:25.459784   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0703 11:31:25.465519   35902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0703 11:31:25.471612   35902 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0703 11:31:25.477517   35902 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0703 11:31:25.482810   35902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0703 11:31:25.563677   35902 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0703 11:31:25.674639   35902 start.go:494] detecting cgroup driver to use...
I0703 11:31:25.674668   35902 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0703 11:31:25.674714   35902 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0703 11:31:25.683181   35902 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0703 11:31:25.683227   35902 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0703 11:31:25.701946   35902 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0703 11:31:25.714001   35902 ssh_runner.go:195] Run: which cri-dockerd
I0703 11:31:25.716644   35902 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0703 11:31:25.723398   35902 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0703 11:31:25.734774   35902 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0703 11:31:25.835239   35902 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0703 11:31:25.916404   35902 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0703 11:31:25.916493   35902 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0703 11:31:25.928137   35902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0703 11:31:26.030561   35902 ssh_runner.go:195] Run: sudo systemctl restart docker
I0703 11:31:26.346770   35902 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0703 11:31:26.354588   35902 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0703 11:31:26.362189   35902 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0703 11:31:26.369151   35902 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0703 11:31:26.470743   35902 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0703 11:31:26.549262   35902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0703 11:31:26.631780   35902 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0703 11:31:26.640393   35902 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0703 11:31:26.647621   35902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0703 11:31:26.730800   35902 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0703 11:31:27.087817   35902 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0703 11:31:27.087886   35902 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0703 11:31:27.090296   35902 start.go:562] Will wait 60s for crictl version
I0703 11:31:27.090335   35902 ssh_runner.go:195] Run: which crictl
I0703 11:31:27.092322   35902 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0703 11:31:27.263467   35902 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0703 11:31:27.263525   35902 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0703 11:31:27.411020   35902 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0703 11:31:27.434038   35902 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0703 11:31:27.434855   35902 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0703 11:31:27.456169   35902 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0703 11:31:27.460339   35902 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0703 11:31:27.470893   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0703 11:31:27.487015   35902 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/chucho:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0703 11:31:27.487134   35902 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0703 11:31:27.487191   35902 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0703 11:31:27.504931   35902 docker.go:685] Got preloaded images: -- stdout --
lautaro124/manage_app:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0703 11:31:27.504941   35902 docker.go:615] Images already preloaded, skipping extraction
I0703 11:31:27.505022   35902 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0703 11:31:27.520704   35902 docker.go:685] Got preloaded images: -- stdout --
lautaro124/manage_app:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0703 11:31:27.520715   35902 cache_images.go:84] Images are preloaded, skipping loading
I0703 11:31:27.520724   35902 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0703 11:31:27.520833   35902 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0703 11:31:27.520883   35902 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0703 11:31:27.819707   35902 cni.go:84] Creating CNI manager for ""
I0703 11:31:27.819719   35902 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0703 11:31:27.819726   35902 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0703 11:31:27.819741   35902 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0703 11:31:27.819824   35902 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0703 11:31:27.820476   35902 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0703 11:31:27.828271   35902 binaries.go:44] Found k8s binaries, skipping transfer
I0703 11:31:27.828310   35902 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0703 11:31:27.833562   35902 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0703 11:31:27.851545   35902 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0703 11:31:27.863637   35902 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0703 11:31:27.875045   35902 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0703 11:31:27.877370   35902 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0703 11:31:27.883830   35902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0703 11:31:27.960774   35902 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0703 11:31:27.969647   35902 certs.go:68] Setting up /home/chucho/.minikube/profiles/minikube for IP: 192.168.49.2
I0703 11:31:27.969655   35902 certs.go:194] generating shared ca certs ...
I0703 11:31:27.969665   35902 certs.go:226] acquiring lock for ca certs: {Name:mk2603a5b1604fce593e0cbdb00caca3bcee4aa1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0703 11:31:27.971974   35902 certs.go:235] skipping valid "minikubeCA" ca cert: /home/chucho/.minikube/ca.key
I0703 11:31:27.972348   35902 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/chucho/.minikube/proxy-client-ca.key
I0703 11:31:27.972371   35902 certs.go:256] generating profile certs ...
I0703 11:31:27.974007   35902 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/chucho/.minikube/profiles/minikube/client.key
I0703 11:31:27.974776   35902 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/chucho/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0703 11:31:27.975637   35902 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/chucho/.minikube/profiles/minikube/proxy-client.key
I0703 11:31:27.975806   35902 certs.go:484] found cert: /home/chucho/.minikube/certs/ca-key.pem (1679 bytes)
I0703 11:31:27.975826   35902 certs.go:484] found cert: /home/chucho/.minikube/certs/ca.pem (1078 bytes)
I0703 11:31:27.975845   35902 certs.go:484] found cert: /home/chucho/.minikube/certs/cert.pem (1123 bytes)
I0703 11:31:27.975857   35902 certs.go:484] found cert: /home/chucho/.minikube/certs/key.pem (1675 bytes)
I0703 11:31:27.977000   35902 ssh_runner.go:362] scp /home/chucho/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0703 11:31:27.994190   35902 ssh_runner.go:362] scp /home/chucho/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0703 11:31:28.009979   35902 ssh_runner.go:362] scp /home/chucho/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0703 11:31:28.024968   35902 ssh_runner.go:362] scp /home/chucho/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0703 11:31:28.041213   35902 ssh_runner.go:362] scp /home/chucho/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0703 11:31:28.058349   35902 ssh_runner.go:362] scp /home/chucho/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0703 11:31:28.074848   35902 ssh_runner.go:362] scp /home/chucho/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0703 11:31:28.091345   35902 ssh_runner.go:362] scp /home/chucho/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0703 11:31:28.107800   35902 ssh_runner.go:362] scp /home/chucho/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0703 11:31:28.124402   35902 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0703 11:31:28.136395   35902 ssh_runner.go:195] Run: openssl version
I0703 11:31:28.153539   35902 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0703 11:31:28.162998   35902 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0703 11:31:28.165489   35902 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul  2 00:49 /usr/share/ca-certificates/minikubeCA.pem
I0703 11:31:28.165595   35902 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0703 11:31:28.171244   35902 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0703 11:31:28.179794   35902 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0703 11:31:28.183003   35902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0703 11:31:28.188679   35902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0703 11:31:28.194698   35902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0703 11:31:28.200519   35902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0703 11:31:28.207192   35902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0703 11:31:28.212890   35902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0703 11:31:28.217667   35902 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/chucho:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0703 11:31:28.217752   35902 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0703 11:31:28.232933   35902 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0703 11:31:28.240191   35902 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0703 11:31:28.240200   35902 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0703 11:31:28.248221   35902 kubeadm.go:587] restartPrimaryControlPlane start ...
I0703 11:31:28.248319   35902 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0703 11:31:28.258363   35902 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0703 11:31:28.258431   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0703 11:31:28.277651   35902 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:60470"
I0703 11:31:28.277667   35902 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:60470, want: 127.0.0.1:51078
I0703 11:31:28.277965   35902 kubeconfig.go:62] /home/chucho/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0703 11:31:28.281516   35902 lock.go:35] WriteFile acquiring /home/chucho/.kube/config: {Name:mk7acc70cc2c512a17c509f0255d02eb419c2788 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0703 11:31:28.306783   35902 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0703 11:31:28.315440   35902 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0703 11:31:28.315461   35902 kubeadm.go:591] duration metric: took 67.226983ms to restartPrimaryControlPlane
I0703 11:31:28.315467   35902 kubeadm.go:393] duration metric: took 97.807923ms to StartCluster
I0703 11:31:28.315480   35902 settings.go:142] acquiring lock: {Name:mk538924ab22f35e82fac2f5b3f79d9051aea5dd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0703 11:31:28.315548   35902 settings.go:150] Updating kubeconfig:  /home/chucho/.kube/config
I0703 11:31:28.316359   35902 lock.go:35] WriteFile acquiring /home/chucho/.kube/config: {Name:mk7acc70cc2c512a17c509f0255d02eb419c2788 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0703 11:31:28.316537   35902 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0703 11:31:28.321441   35902 out.go:177] 🔎  Verifying Kubernetes components...
I0703 11:31:28.316657   35902 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0703 11:31:28.316710   35902 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0703 11:31:28.323651   35902 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0703 11:31:28.323653   35902 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0703 11:31:28.323668   35902 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0703 11:31:28.323668   35902 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0703 11:31:28.323672   35902 addons.go:243] addon storage-provisioner should already be in state true
I0703 11:31:28.323681   35902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0703 11:31:28.323687   35902 host.go:66] Checking if "minikube" exists ...
I0703 11:31:28.323832   35902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0703 11:31:28.323918   35902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0703 11:31:28.348307   35902 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0703 11:31:28.380219   35902 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W0703 11:31:28.380219   35902 addons.go:243] addon default-storageclass should already be in state true
I0703 11:31:28.380254   35902 host.go:66] Checking if "minikube" exists ...
I0703 11:31:28.382863   35902 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0703 11:31:28.382870   35902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0703 11:31:28.382922   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:28.383085   35902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0703 11:31:28.408523   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:28.414028   35902 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0703 11:31:28.414040   35902 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0703 11:31:28.414140   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0703 11:31:28.436634   35902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51074 SSHKeyPath:/home/chucho/.minikube/machines/minikube/id_rsa Username:docker}
I0703 11:31:28.460871   35902 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0703 11:31:28.470653   35902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0703 11:31:28.490492   35902 api_server.go:52] waiting for apiserver process to appear ...
I0703 11:31:28.490570   35902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0703 11:31:28.530253   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0703 11:31:28.533085   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0703 11:31:28.990580   35902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0703 11:31:28.991373   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:28.991393   35902 retry.go:31] will retry after 132.206499ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0703 11:31:28.991436   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:28.991440   35902 retry.go:31] will retry after 229.549204ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.124035   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0703 11:31:29.188474   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.188494   35902 retry.go:31] will retry after 515.684464ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.221822   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0703 11:31:29.304933   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.304952   35902 retry.go:31] will retry after 410.883686ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.491129   35902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0703 11:31:29.704809   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0703 11:31:29.716326   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0703 11:31:29.776231   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.776254   35902 retry.go:31] will retry after 671.758553ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0703 11:31:29.793661   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.793680   35902 retry.go:31] will retry after 361.358599ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:29.991229   35902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0703 11:31:29.999897   35902 api_server.go:72] duration metric: took 1.683338421s to wait for apiserver process to appear ...
I0703 11:31:29.999911   35902 api_server.go:88] waiting for apiserver healthz status ...
I0703 11:31:29.999926   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
I0703 11:31:30.002420   35902 api_server.go:269] stopped: https://127.0.0.1:51078/healthz: Get "https://127.0.0.1:51078/healthz": EOF
I0703 11:31:30.155807   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0703 11:31:30.211293   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:30.211313   35902 retry.go:31] will retry after 1.180635588s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:30.448727   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0703 11:31:30.500188   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
W0703 11:31:30.503923   35902 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:30.503951   35902 retry.go:31] will retry after 572.466205ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0703 11:31:30.551212   35902 api_server.go:269] stopped: https://127.0.0.1:51078/healthz: Get "https://127.0.0.1:51078/healthz": EOF
I0703 11:31:31.000791   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
I0703 11:31:31.077337   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0703 11:31:31.393086   35902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0703 11:31:32.699591   35902 api_server.go:279] https://127.0.0.1:51078/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0703 11:31:32.700347   35902 api_server.go:103] status: https://127.0.0.1:51078/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0703 11:31:32.700372   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
I0703 11:31:32.728532   35902 api_server.go:279] https://127.0.0.1:51078/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0703 11:31:32.728549   35902 api_server.go:103] status: https://127.0.0.1:51078/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0703 11:31:33.000766   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
I0703 11:31:33.003775   35902 api_server.go:279] https://127.0.0.1:51078/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0703 11:31:33.003786   35902 api_server.go:103] status: https://127.0.0.1:51078/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0703 11:31:33.116139   35902 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.038774582s)
I0703 11:31:33.116224   35902 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.723125377s)
I0703 11:31:33.166150   35902 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0703 11:31:33.167741   35902 addons.go:505] duration metric: took 4.851084491s for enable addons: enabled=[storage-provisioner default-storageclass]
I0703 11:31:33.500474   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
I0703 11:31:33.503398   35902 api_server.go:279] https://127.0.0.1:51078/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0703 11:31:33.503409   35902 api_server.go:103] status: https://127.0.0.1:51078/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0703 11:31:34.000907   35902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51078/healthz ...
I0703 11:31:34.004055   35902 api_server.go:279] https://127.0.0.1:51078/healthz returned 200:
ok
I0703 11:31:34.005825   35902 api_server.go:141] control plane version: v1.30.0
I0703 11:31:34.005837   35902 api_server.go:131] duration metric: took 4.005921517s to wait for apiserver health ...
I0703 11:31:34.006841   35902 system_pods.go:43] waiting for kube-system pods to appear ...
I0703 11:31:34.033116   35902 system_pods.go:59] 7 kube-system pods found
I0703 11:31:34.033142   35902 system_pods.go:61] "coredns-7db6d8ff4d-8zjnv" [019a74c1-b33a-4ab4-b4af-fb6b69400bbb] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0703 11:31:34.033147   35902 system_pods.go:61] "etcd-minikube" [2af80f8b-6ad7-4fc5-a18a-cbead0b7ac93] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0703 11:31:34.033153   35902 system_pods.go:61] "kube-apiserver-minikube" [ad45b8a0-fa2c-4be0-bf06-dd7b4d2648b2] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0703 11:31:34.033157   35902 system_pods.go:61] "kube-controller-manager-minikube" [51384677-48f4-484f-9e9a-530086f9028a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0703 11:31:34.033160   35902 system_pods.go:61] "kube-proxy-w2dxj" [abe5a95c-4194-40ed-bc0d-8448e2a11d5b] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0703 11:31:34.033177   35902 system_pods.go:61] "kube-scheduler-minikube" [05b89ede-5776-4aba-96dd-d2757e8ab0b1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0703 11:31:34.033180   35902 system_pods.go:61] "storage-provisioner" [08413a4f-4e3f-41ab-9056-2c7450d070d3] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0703 11:31:34.033185   35902 system_pods.go:74] duration metric: took 26.338946ms to wait for pod list to return data ...
I0703 11:31:34.033194   35902 kubeadm.go:576] duration metric: took 5.716641419s to wait for: map[apiserver:true system_pods:true]
I0703 11:31:34.033204   35902 node_conditions.go:102] verifying NodePressure condition ...
I0703 11:31:34.037101   35902 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0703 11:31:34.038249   35902 node_conditions.go:123] node cpu capacity is 3
I0703 11:31:34.038722   35902 node_conditions.go:105] duration metric: took 5.512946ms to run NodePressure ...
I0703 11:31:34.038732   35902 start.go:240] waiting for startup goroutines ...
I0703 11:31:34.038735   35902 start.go:245] waiting for cluster config update ...
I0703 11:31:34.039924   35902 start.go:254] writing updated cluster config ...
I0703 11:31:34.040141   35902 ssh_runner.go:195] Run: rm -f paused
I0703 11:31:34.693998   35902 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0703 11:31:34.917788   35902 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.032587663Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.032606883Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.032660365Z" level=info msg="Daemon has completed initialization"
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.037797350Z" level=info msg="Processing signal 'terminated'"
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.064945911Z" level=info msg="API listen on /var/run/docker.sock"
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.065002552Z" level=info msg="API listen on [::]:2376"
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.066481086Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jul 03 14:31:26 minikube dockerd[762]: time="2024-07-03T14:31:26.066982146Z" level=info msg="Daemon shutdown complete"
Jul 03 14:31:26 minikube systemd[1]: docker.service: Deactivated successfully.
Jul 03 14:31:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Jul 03 14:31:26 minikube systemd[1]: Starting Docker Application Container Engine...
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.115388608Z" level=info msg="Starting up"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.131977300Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.148950144Z" level=info msg="Loading containers: start."
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.278359261Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.299596131Z" level=info msg="Loading containers: done."
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.312565673Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.312594804Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.312600595Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.312604415Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.312623795Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.312669257Z" level=info msg="Daemon has completed initialization"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.345152028Z" level=info msg="API listen on [::]:2376"
Jul 03 14:31:26 minikube dockerd[983]: time="2024-07-03T14:31:26.345169159Z" level=info msg="API listen on /var/run/docker.sock"
Jul 03 14:31:26 minikube systemd[1]: Started Docker Application Container Engine.
Jul 03 14:31:26 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 03 14:31:26 minikube cri-dockerd[1232]: time="2024-07-03T14:31:26Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jul 03 14:31:26 minikube cri-dockerd[1232]: time="2024-07-03T14:31:26Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 03 14:31:26 minikube cri-dockerd[1232]: time="2024-07-03T14:31:26Z" level=info msg="Start docker client with request timeout 0s"
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Loaded network plugin cni"
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Docker cri networking managed by network plugin cni"
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Setting cgroupDriver cgroupfs"
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 03 14:31:27 minikube cri-dockerd[1232]: time="2024-07-03T14:31:27Z" level=info msg="Start cri-dockerd grpc backend"
Jul 03 14:31:27 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 03 14:31:28 minikube cri-dockerd[1232]: time="2024-07-03T14:31:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8zjnv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"065720481d9d73d8b5890bb98adddb6e587072a1f28b4c4e985076810df61e76\""
Jul 03 14:31:28 minikube cri-dockerd[1232]: time="2024-07-03T14:31:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8zjnv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a4f8d4d828c10f1081f321c56b5a9dc4d0a16a124484e96423599470ef36c9b8\""
Jul 03 14:31:29 minikube cri-dockerd[1232]: time="2024-07-03T14:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ec65a154fe561a2b6d046d8f31bfc83fd6360776c993ee2f06f8bc1f0b7e5fb1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:29 minikube cri-dockerd[1232]: time="2024-07-03T14:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8c2846fb1dbd26699d1c3ce169d27ba56b1ff52c0c5bf60da33398a443b897c9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:29 minikube cri-dockerd[1232]: time="2024-07-03T14:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9f25c374ab4b932b35f3242f719b45652449e4ab9c9e0c163d488ed024e7d589/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:29 minikube cri-dockerd[1232]: time="2024-07-03T14:31:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95ff118ec917cfc7a734024792fc39bf0d424ee7eee18e3caa3b56d705ece6a1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:30 minikube cri-dockerd[1232]: time="2024-07-03T14:31:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8zjnv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"065720481d9d73d8b5890bb98adddb6e587072a1f28b4c4e985076810df61e76\""
Jul 03 14:31:32 minikube cri-dockerd[1232]: time="2024-07-03T14:31:32Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 03 14:31:34 minikube cri-dockerd[1232]: time="2024-07-03T14:31:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1a5faa8bf89d685c936f410e24fcfdd8e7e251441615fa1d0a238176daa20302/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:34 minikube cri-dockerd[1232]: time="2024-07-03T14:31:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09c50cfe1da7e69cde36993699ed999946f90712af7f0feb26fd1cb1763eae60/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:34 minikube cri-dockerd[1232]: time="2024-07-03T14:31:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/42bb2d0fc2d32c2d0fe22bc3a63dbd209953678443e57bbfa30fcd5c4d806b81/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 03 14:31:35 minikube dockerd[983]: time="2024-07-03T14:31:35.039450919Z" level=info msg="ignoring event" container=5fa3962cd30729685033ca79b2d8bf8ee398e77f33854184448b3caea6416693 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 03 14:40:46 minikube cri-dockerd[1232]: time="2024-07-03T14:40:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/41e12b456c1f02e2984bec6e4a17f900d7ecce1d6fdab7e72649240ae3596823/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 03 14:40:48 minikube dockerd[983]: time="2024-07-03T14:40:48.522460906Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=88a75ca43c211e58 traceID=3437a031fb6e9d64aaac51ef77dc0f23
Jul 03 14:41:00 minikube cri-dockerd[1232]: time="2024-07-03T14:41:00Z" level=info msg="Pulling image mongo:latest: 403f753f5920: Downloading [=============================================>     ]  212.7MB/233.9MB"
Jul 03 14:41:05 minikube cri-dockerd[1232]: time="2024-07-03T14:41:05Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Jul 03 14:41:09 minikube dockerd[983]: time="2024-07-03T14:41:09.496744544Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=84cac35be87086b3 traceID=e9cc18777977a53e51a447a07c64755c
Jul 03 14:41:41 minikube dockerd[983]: time="2024-07-03T14:41:41.755061591Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=0010c3f4229b949a traceID=2ae9f17aed1ea5c17dcda4d5a2f0c44e
Jul 03 14:42:31 minikube dockerd[983]: time="2024-07-03T14:42:31.818618737Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=5e994be7bfbb1bcf traceID=f41bdf9c99fd30363f33d5eec0809c59
Jul 03 14:43:55 minikube dockerd[983]: time="2024-07-03T14:43:55.809440084Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=52c75828db70d8a2 traceID=7550df54a6f2ba41b83b55da1dbe27db
Jul 03 14:46:41 minikube dockerd[983]: time="2024-07-03T14:46:41.809267783Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=bb1d945940f813f3 traceID=4eab0e4d5c0fcc9b7105e70851a2dd8c
Jul 03 14:51:54 minikube dockerd[983]: time="2024-07-03T14:51:54.823269540Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=1181d3b9820525dd traceID=cb4ba699a974f5dd4a91aaf2c94bd3a2
Jul 03 14:57:04 minikube dockerd[983]: time="2024-07-03T14:57:04.903154233Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown" spanID=f8218657ca1dc1e0 traceID=3733bcfb38091069e2f6e91b585ed277


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4e8278dba0703       mongo@sha256:1cd3951000020c1cb1757868e6cfd82667f57d80bb31fed8b585e26a8a1d960f   18 minutes ago      Running             mongo                     0                   41e12b456c1f0       manage-app-v1-65dcb4445b-xh6z9
52da41a492d7a       6e38f40d628db                                                                   28 minutes ago      Running             storage-provisioner       9                   09c50cfe1da7e       storage-provisioner
a352a6a20e46a       cbb01a7bd410d                                                                   28 minutes ago      Running             coredns                   2                   42bb2d0fc2d32       coredns-7db6d8ff4d-8zjnv
5fa3962cd3072       6e38f40d628db                                                                   28 minutes ago      Exited              storage-provisioner       8                   09c50cfe1da7e       storage-provisioner
9ad9beb122cdb       a0bf559e280cf                                                                   28 minutes ago      Running             kube-proxy                2                   1a5faa8bf89d6       kube-proxy-w2dxj
dbb49555632af       c7aad43836fa5                                                                   28 minutes ago      Running             kube-controller-manager   2                   ec65a154fe561       kube-controller-manager-minikube
95964baf7e61f       3861cfcd7c04c                                                                   28 minutes ago      Running             etcd                      2                   95ff118ec917c       etcd-minikube
61bd8084c64ab       c42f13656d0b2                                                                   28 minutes ago      Running             kube-apiserver            2                   9f25c374ab4b9       kube-apiserver-minikube
e9402540f571a       259c8277fcbbc                                                                   28 minutes ago      Running             kube-scheduler            2                   8c2846fb1dbd2       kube-scheduler-minikube
2cad87fd13fca       cbb01a7bd410d                                                                   27 hours ago        Exited              coredns                   1                   065720481d9d7       coredns-7db6d8ff4d-8zjnv
5ba6afcb37888       a0bf559e280cf                                                                   27 hours ago        Exited              kube-proxy                1                   948e903ef0dbb       kube-proxy-w2dxj
3a5e87980c9f2       c42f13656d0b2                                                                   27 hours ago        Exited              kube-apiserver            1                   58c42f7603347       kube-apiserver-minikube
a4e8519f1aa00       c7aad43836fa5                                                                   27 hours ago        Exited              kube-controller-manager   1                   4be36d31a3d96       kube-controller-manager-minikube
ad9f7dc6fdcbd       3861cfcd7c04c                                                                   27 hours ago        Exited              etcd                      1                   b44471f2a7b0f       etcd-minikube
919a141190dd4       259c8277fcbbc                                                                   27 hours ago        Exited              kube-scheduler            1                   572a5b48e53e3       kube-scheduler-minikube


==> coredns [2cad87fd13fc] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:39573 - 26142 "HINFO IN 937979044280027166.8378752185486738912. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.083890962s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.295138253s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [a352a6a20e46] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:56901 - 48618 "HINFO IN 8384860801393815125.1476963868895716586. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.038321121s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_07_01T21_49_42_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Jul 2024 00:49:39 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 03 Jul 2024 14:59:59 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 03 Jul 2024 14:56:52 +0000   Tue, 02 Jul 2024 00:49:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 03 Jul 2024 14:56:52 +0000   Tue, 02 Jul 2024 00:49:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 03 Jul 2024 14:56:52 +0000   Tue, 02 Jul 2024 00:49:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 03 Jul 2024 14:56:52 +0000   Tue, 02 Jul 2024 00:49:41 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                3
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4015216Ki
  pods:               110
Allocatable:
  cpu:                3
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4015216Ki
  pods:               110
System Info:
  Machine ID:                 7985fdc035634443a346196b41519b32
  System UUID:                7985fdc035634443a346196b41519b32
  Boot ID:                    60fd2912-0dda-4410-8d99-5b0f302b44ad
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     manage-app-v1-65dcb4445b-xh6z9      1 (33%!)(MISSING)       1 (33%!)(MISSING)     512Mi (13%!)(MISSING)      512Mi (13%!)(MISSING)    19m
  kube-system                 coredns-7db6d8ff4d-8zjnv            100m (3%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     38h
  kube-system                 etcd-minikube                       100m (3%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         38h
  kube-system                 kube-apiserver-minikube             250m (8%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 kube-controller-manager-minikube    200m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 kube-proxy-w2dxj                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 kube-scheduler-minikube             100m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1750m (58%!)(MISSING)  1 (33%!)(MISSING)
  memory             682Mi (17%!)(MISSING)  682Mi (17%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 28m                kube-proxy       
  Normal  Starting                 28m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  28m (x8 over 28m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    28m (x8 over 28m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     28m (x7 over 28m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  28m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           28m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000265] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000259] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000260] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.821766] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.556209] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[Jul 3 14:28] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.024240] WSL (1) WARNING: /usr/share/zoneinfo/America/Buenos_Aires not found. Is the tzdata package installed?
[  +0.082466] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000445] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000331] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000479] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.025157] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.074836] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000515] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000307] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000384] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000346] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000289] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000299] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000423] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000326] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000357] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000293] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000400] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000415] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000310] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.043355] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +3.296584] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000800] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000559] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.002597] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.010880] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000786] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.022178] WSL (1) WARNING: /usr/share/zoneinfo/America/Buenos_Aires not found. Is the tzdata package installed?
[  +0.032138] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000471] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000407] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000474] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.007288] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.042484] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000394] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000363] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000348] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000312] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000303] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000311] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000274] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000274] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000273] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000287] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000294] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000286] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000281] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.025489] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.461744] new mount options do not match the existing superblock, will be ignored
[  +0.000137] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [95964baf7e61] <==
{"level":"warn","ts":"2024-07-03T14:54:47.168465Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:46.784377Z","time spent":"384.039129ms","remote":"127.0.0.1:38214","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14879 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-07-03T14:54:49.916816Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"535.530459ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:49.916869Z","caller":"traceutil/trace.go:171","msg":"trace[1232484821] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14881; }","duration":"535.613324ms","start":"2024-07-03T14:54:49.381244Z","end":"2024-07-03T14:54:49.916857Z","steps":["trace[1232484821] 'range keys from in-memory index tree'  (duration: 535.485628ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:49.916896Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:49.381231Z","time spent":"535.659316ms","remote":"127.0.0.1:38040","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-07-03T14:54:50.176212Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"571.345678ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030276895447814 > lease_revoke:<id:70cc907901f91acc>","response":"size:29"}
{"level":"info","ts":"2024-07-03T14:54:50.176285Z","caller":"traceutil/trace.go:171","msg":"trace[221382653] linearizableReadLoop","detail":"{readStateIndex:18555; appliedIndex:18554; }","duration":"509.368019ms","start":"2024-07-03T14:54:49.666907Z","end":"2024-07-03T14:54:50.176275Z","steps":["trace[221382653] 'read index received'  (duration: 47.567µs)","trace[221382653] 'applied index is now lower than readState.Index'  (duration: 509.319822ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-03T14:54:50.176489Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"509.5798ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-07-03T14:54:50.17661Z","caller":"traceutil/trace.go:171","msg":"trace[2053680217] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:14881; }","duration":"509.735928ms","start":"2024-07-03T14:54:49.666866Z","end":"2024-07-03T14:54:50.176602Z","steps":["trace[2053680217] 'agreement among raft nodes before linearized reading'  (duration: 509.579079ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:50.17664Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:49.666849Z","time spent":"509.77966ms","remote":"127.0.0.1:38222","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":31,"request content":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true "}
{"level":"warn","ts":"2024-07-03T14:54:50.176761Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"259.24971ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:50.176779Z","caller":"traceutil/trace.go:171","msg":"trace[2097136332] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14881; }","duration":"259.283711ms","start":"2024-07-03T14:54:49.917491Z","end":"2024-07-03T14:54:50.176774Z","steps":["trace[2097136332] 'agreement among raft nodes before linearized reading'  (duration: 259.25253ms)"],"step_count":1}
{"level":"info","ts":"2024-07-03T14:54:51.432111Z","caller":"traceutil/trace.go:171","msg":"trace[704375040] transaction","detail":"{read_only:false; response_revision:14882; number_of_response:1; }","duration":"168.916752ms","start":"2024-07-03T14:54:51.263181Z","end":"2024-07-03T14:54:51.432098Z","steps":["trace[704375040] 'process raft request'  (duration: 168.651188ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:52.028836Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"384.499328ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/mongo-pvc\" ","response":"range_response_count:1 size:1601"}
{"level":"info","ts":"2024-07-03T14:54:52.028891Z","caller":"traceutil/trace.go:171","msg":"trace[494817254] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/mongo-pvc; range_end:; response_count:1; response_revision:14882; }","duration":"384.584953ms","start":"2024-07-03T14:54:51.644295Z","end":"2024-07-03T14:54:52.02888Z","steps":["trace[494817254] 'range keys from in-memory index tree'  (duration: 384.372002ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:52.028914Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:51.644283Z","time spent":"384.625645ms","remote":"127.0.0.1:38208","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":1625,"request content":"key:\"/registry/persistentvolumeclaims/default/mongo-pvc\" "}
{"level":"info","ts":"2024-07-03T14:54:53.412057Z","caller":"traceutil/trace.go:171","msg":"trace[836311417] transaction","detail":"{read_only:false; response_revision:14883; number_of_response:1; }","duration":"269.598872ms","start":"2024-07-03T14:54:53.142444Z","end":"2024-07-03T14:54:53.412043Z","steps":["trace[836311417] 'process raft request'  (duration: 269.3593ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:53.751549Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"315.390144ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-07-03T14:54:53.751787Z","caller":"traceutil/trace.go:171","msg":"trace[1984208390] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:14883; }","duration":"315.651937ms","start":"2024-07-03T14:54:53.436114Z","end":"2024-07-03T14:54:53.751766Z","steps":["trace[1984208390] 'range keys from in-memory index tree'  (duration: 315.265823ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:53.751838Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:53.436098Z","time spent":"315.730541ms","remote":"127.0.0.1:38214","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-07-03T14:54:54.249356Z","caller":"traceutil/trace.go:171","msg":"trace[2011074664] linearizableReadLoop","detail":"{readStateIndex:18558; appliedIndex:18557; }","duration":"416.0182ms","start":"2024-07-03T14:54:53.833323Z","end":"2024-07-03T14:54:54.249341Z","steps":["trace[2011074664] 'read index received'  (duration: 415.883634ms)","trace[2011074664] 'applied index is now lower than readState.Index'  (duration: 134.106µs)"],"step_count":2}
{"level":"info","ts":"2024-07-03T14:54:54.249441Z","caller":"traceutil/trace.go:171","msg":"trace[617565672] transaction","detail":"{read_only:false; response_revision:14884; number_of_response:1; }","duration":"495.049612ms","start":"2024-07-03T14:54:53.75438Z","end":"2024-07-03T14:54:54.24943Z","steps":["trace[617565672] 'process raft request'  (duration: 494.864442ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:54.249479Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"416.150847ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2024-07-03T14:54:54.2495Z","caller":"traceutil/trace.go:171","msg":"trace[135308877] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:14884; }","duration":"416.212081ms","start":"2024-07-03T14:54:53.833283Z","end":"2024-07-03T14:54:54.249495Z","steps":["trace[135308877] 'agreement among raft nodes before linearized reading'  (duration: 416.135687ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:54.249508Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:53.754364Z","time spent":"495.102285ms","remote":"127.0.0.1:38214","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14882 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-07-03T14:54:54.249519Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:53.833269Z","time spent":"416.246562ms","remote":"127.0.0.1:38056","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":157,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2024-07-03T14:54:55.135381Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"433.858208ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030276895447833 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc907901f91b18>","response":"size:41"}
{"level":"info","ts":"2024-07-03T14:54:55.135518Z","caller":"traceutil/trace.go:171","msg":"trace[1168143662] linearizableReadLoop","detail":"{readStateIndex:18560; appliedIndex:18558; }","duration":"760.150638ms","start":"2024-07-03T14:54:54.375357Z","end":"2024-07-03T14:54:55.135508Z","steps":["trace[1168143662] 'read index received'  (duration: 326.162803ms)","trace[1168143662] 'applied index is now lower than readState.Index'  (duration: 433.987314ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-03T14:54:55.135568Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:54.25096Z","time spent":"884.606341ms","remote":"127.0.0.1:38056","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-07-03T14:54:55.135726Z","caller":"traceutil/trace.go:171","msg":"trace[1348532177] transaction","detail":"{read_only:false; response_revision:14885; number_of_response:1; }","duration":"820.475536ms","start":"2024-07-03T14:54:54.315243Z","end":"2024-07-03T14:54:55.135718Z","steps":["trace[1348532177] 'process raft request'  (duration: 820.203662ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:55.135779Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:54.315224Z","time spent":"820.516098ms","remote":"127.0.0.1:54950","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:14877 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-07-03T14:54:55.135856Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"760.508786ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-07-03T14:54:55.135875Z","caller":"traceutil/trace.go:171","msg":"trace[1052369519] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:14885; }","duration":"760.554338ms","start":"2024-07-03T14:54:54.375317Z","end":"2024-07-03T14:54:55.135871Z","steps":["trace[1052369519] 'agreement among raft nodes before linearized reading'  (duration: 760.519366ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:55.1359Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:54.375301Z","time spent":"760.595281ms","remote":"127.0.0.1:55172","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":31,"request content":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true "}
{"level":"warn","ts":"2024-07-03T14:54:55.135921Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"753.897526ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:55.13595Z","caller":"traceutil/trace.go:171","msg":"trace[815175841] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14885; }","duration":"753.958859ms","start":"2024-07-03T14:54:54.381985Z","end":"2024-07-03T14:54:55.135944Z","steps":["trace[815175841] 'agreement among raft nodes before linearized reading'  (duration: 753.915907ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:55.135972Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:54.381972Z","time spent":"753.99669ms","remote":"127.0.0.1:38040","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-07-03T14:54:55.45151Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"196.785896ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:55.451825Z","caller":"traceutil/trace.go:171","msg":"trace[340126650] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:14886; }","duration":"197.080731ms","start":"2024-07-03T14:54:55.254676Z","end":"2024-07-03T14:54:55.451757Z","steps":["trace[340126650] 'count revisions from in-memory index tree'  (duration: 196.732543ms)"],"step_count":1}
{"level":"info","ts":"2024-07-03T14:54:56.642502Z","caller":"traceutil/trace.go:171","msg":"trace[1521037017] linearizableReadLoop","detail":"{readStateIndex:18562; appliedIndex:18561; }","duration":"261.44871ms","start":"2024-07-03T14:54:56.38104Z","end":"2024-07-03T14:54:56.642489Z","steps":["trace[1521037017] 'read index received'  (duration: 261.322084ms)","trace[1521037017] 'applied index is now lower than readState.Index'  (duration: 126.226µs)"],"step_count":2}
{"level":"warn","ts":"2024-07-03T14:54:56.642604Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"261.546006ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:56.642622Z","caller":"traceutil/trace.go:171","msg":"trace[1844586771] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14887; }","duration":"261.600829ms","start":"2024-07-03T14:54:56.381015Z","end":"2024-07-03T14:54:56.642616Z","steps":["trace[1844586771] 'agreement among raft nodes before linearized reading'  (duration: 261.529535ms)"],"step_count":1}
{"level":"info","ts":"2024-07-03T14:54:56.642681Z","caller":"traceutil/trace.go:171","msg":"trace[1821239201] transaction","detail":"{read_only:false; response_revision:14887; number_of_response:1; }","duration":"387.983573ms","start":"2024-07-03T14:54:56.254688Z","end":"2024-07-03T14:54:56.642671Z","steps":["trace[1821239201] 'process raft request'  (duration: 387.70669ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:56.642741Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:54:56.254669Z","time spent":"388.033936ms","remote":"127.0.0.1:38214","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14884 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-07-03T14:54:57.594675Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"213.772918ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:57.597676Z","caller":"traceutil/trace.go:171","msg":"trace[1609767996] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14887; }","duration":"213.855742ms","start":"2024-07-03T14:54:57.380861Z","end":"2024-07-03T14:54:57.594716Z","steps":["trace[1609767996] 'range keys from in-memory index tree'  (duration: 213.734406ms)"],"step_count":1}
{"level":"info","ts":"2024-07-03T14:54:58.842628Z","caller":"traceutil/trace.go:171","msg":"trace[1238059548] transaction","detail":"{read_only:false; response_revision:14888; number_of_response:1; }","duration":"195.210893ms","start":"2024-07-03T14:54:58.647402Z","end":"2024-07-03T14:54:58.842612Z","steps":["trace[1238059548] 'process raft request'  (duration: 195.089487ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:59.116624Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.988117ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-03T14:54:59.116687Z","caller":"traceutil/trace.go:171","msg":"trace[1223103552] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:14888; }","duration":"137.092022ms","start":"2024-07-03T14:54:58.979584Z","end":"2024-07-03T14:54:59.116676Z","steps":["trace[1223103552] 'count revisions from in-memory index tree'  (duration: 136.920293ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:54:59.555931Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"293.685385ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030276895447853 > lease_revoke:<id:70cc907901f91af0>","response":"size:29"}
{"level":"info","ts":"2024-07-03T14:54:59.556022Z","caller":"traceutil/trace.go:171","msg":"trace[674973887] linearizableReadLoop","detail":"{readStateIndex:18564; appliedIndex:18563; }","duration":"285.904116ms","start":"2024-07-03T14:54:59.270103Z","end":"2024-07-03T14:54:59.556007Z","steps":["trace[674973887] 'read index received'  (duration: 52.313µs)","trace[674973887] 'applied index is now lower than readState.Index'  (duration: 285.850563ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-03T14:54:59.556111Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"174.734887ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-07-03T14:54:59.556114Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"286.023031ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-07-03T14:54:59.55613Z","caller":"traceutil/trace.go:171","msg":"trace[71186825] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14888; }","duration":"174.796111ms","start":"2024-07-03T14:54:59.381328Z","end":"2024-07-03T14:54:59.556124Z","steps":["trace[71186825] 'agreement among raft nodes before linearized reading'  (duration: 174.752728ms)"],"step_count":1}
{"level":"info","ts":"2024-07-03T14:54:59.556132Z","caller":"traceutil/trace.go:171","msg":"trace[1640215387] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:14888; }","duration":"286.076585ms","start":"2024-07-03T14:54:59.270052Z","end":"2024-07-03T14:54:59.556128Z","steps":["trace[1640215387] 'agreement among raft nodes before linearized reading'  (duration: 286.020102ms)"],"step_count":1}
{"level":"info","ts":"2024-07-03T14:55:03.339245Z","caller":"traceutil/trace.go:171","msg":"trace[36463879] transaction","detail":"{read_only:false; response_revision:14890; number_of_response:1; }","duration":"410.415967ms","start":"2024-07-03T14:55:02.928812Z","end":"2024-07-03T14:55:03.339228Z","steps":["trace[36463879] 'process raft request'  (duration: 410.28535ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-03T14:55:03.339374Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-03T14:55:02.928799Z","time spent":"410.506142ms","remote":"127.0.0.1:38214","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14889 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-07-03T14:56:31.601277Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14723}
{"level":"info","ts":"2024-07-03T14:56:31.603506Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":14723,"took":"2.07867ms","hash":2464692708,"current-db-size-bytes":2564096,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1679360,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-07-03T14:56:31.603537Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2464692708,"revision":14723,"compact-revision":14472}
{"level":"info","ts":"2024-07-03T15:00:04.982222Z","caller":"traceutil/trace.go:171","msg":"trace[521565181] transaction","detail":"{read_only:false; response_revision:15137; number_of_response:1; }","duration":"275.585707ms","start":"2024-07-03T15:00:04.706617Z","end":"2024-07-03T15:00:04.982203Z","steps":["trace[521565181] 'process raft request'  (duration: 275.219989ms)"],"step_count":1}


==> etcd [ad9f7dc6fdcb] <==
{"level":"info","ts":"2024-07-02T14:07:24.322841Z","caller":"traceutil/trace.go:171","msg":"trace[2034059089] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:11653; }","duration":"4.996793206s","start":"2024-07-02T14:07:19.326022Z","end":"2024-07-02T14:07:24.322815Z","steps":["trace[2034059089] 'agreement among raft nodes before linearized reading'  (duration: 4.978701329s)","trace[2034059089] 'range keys from bolt db'  (duration: 17.950881ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-02T14:07:24.322881Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-02T14:07:19.326007Z","time spent":"4.996861859s","remote":"127.0.0.1:58110","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-07-02T14:07:24.32312Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.289585848s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-07-02T14:07:24.323146Z","caller":"traceutil/trace.go:171","msg":"trace[688565198] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:11653; }","duration":"1.289638591s","start":"2024-07-02T14:07:23.033499Z","end":"2024-07-02T14:07:24.323137Z","steps":["trace[688565198] 'agreement among raft nodes before linearized reading'  (duration: 1.272122198s)","trace[688565198] 'range keys from bolt db'  (duration: 17.478071ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-02T14:07:24.323163Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-02T14:07:23.033479Z","time spent":"1.289681052s","remote":"127.0.0.1:57968","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":155,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"info","ts":"2024-07-02T14:07:46.644909Z","caller":"traceutil/trace.go:171","msg":"trace[93627703] transaction","detail":"{read_only:false; response_revision:11672; number_of_response:1; }","duration":"128.628072ms","start":"2024-07-02T14:07:46.516259Z","end":"2024-07-02T14:07:46.644887Z","steps":["trace[93627703] 'process raft request'  (duration: 128.502246ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-02T14:07:46.949154Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.641892ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-02T14:07:47.115232Z","caller":"traceutil/trace.go:171","msg":"trace[821242377] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:11672; }","duration":"377.706115ms","start":"2024-07-02T14:07:46.737484Z","end":"2024-07-02T14:07:47.11519Z","steps":["trace[821242377] 'range keys from in-memory index tree'  (duration: 211.627751ms)"],"step_count":1}
{"level":"info","ts":"2024-07-02T14:08:55.569617Z","caller":"traceutil/trace.go:171","msg":"trace[1014666126] transaction","detail":"{read_only:false; response_revision:11726; number_of_response:1; }","duration":"310.703136ms","start":"2024-07-02T14:08:55.258897Z","end":"2024-07-02T14:08:55.5696Z","steps":["trace[1014666126] 'process raft request'  (duration: 310.604908ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-02T14:08:55.569733Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-02T14:08:55.25888Z","time spent":"310.787353ms","remote":"127.0.0.1:58206","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:11718 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-07-02T14:09:06.802104Z","caller":"traceutil/trace.go:171","msg":"trace[492986794] transaction","detail":"{read_only:false; response_revision:11735; number_of_response:1; }","duration":"257.597322ms","start":"2024-07-02T14:09:06.54449Z","end":"2024-07-02T14:09:06.802087Z","steps":["trace[492986794] 'process raft request'  (duration: 257.506948ms)"],"step_count":1}
{"level":"info","ts":"2024-07-02T14:09:11.232074Z","caller":"traceutil/trace.go:171","msg":"trace[1467226860] transaction","detail":"{read_only:false; response_revision:11738; number_of_response:1; }","duration":"171.003702ms","start":"2024-07-02T14:09:11.061051Z","end":"2024-07-02T14:09:11.232055Z","steps":["trace[1467226860] 'process raft request'  (duration: 170.879778ms)"],"step_count":1}
{"level":"info","ts":"2024-07-02T14:09:13.456683Z","caller":"traceutil/trace.go:171","msg":"trace[924861323] transaction","detail":"{read_only:false; response_revision:11739; number_of_response:1; }","duration":"424.162156ms","start":"2024-07-02T14:09:13.032505Z","end":"2024-07-02T14:09:13.456667Z","steps":["trace[924861323] 'process raft request'  (duration: 422.694553ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-02T14:09:13.456775Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-02T14:09:13.032495Z","time spent":"424.225939ms","remote":"127.0.0.1:57968","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:11732 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128030252840322686 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-07-02T14:09:13.45714Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.742525ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-02T14:09:13.45833Z","caller":"traceutil/trace.go:171","msg":"trace[211118204] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11739; }","duration":"397.810617ms","start":"2024-07-02T14:09:13.059361Z","end":"2024-07-02T14:09:13.457172Z","steps":["trace[211118204] 'agreement among raft nodes before linearized reading'  (duration: 397.719013ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-02T14:09:13.458369Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-02T14:09:13.059348Z","time spent":"399.009435ms","remote":"127.0.0.1:57938","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-07-02T14:09:13.458414Z","caller":"traceutil/trace.go:171","msg":"trace[1520609864] linearizableReadLoop","detail":"{readStateIndex:14637; appliedIndex:14636; }","duration":"397.533088ms","start":"2024-07-02T14:09:13.059387Z","end":"2024-07-02T14:09:13.45692Z","steps":["trace[1520609864] 'read index received'  (duration: 395.745474ms)","trace[1520609864] 'applied index is now lower than readState.Index'  (duration: 1.786784ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-02T14:09:13.458496Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"223.950343ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-07-02T14:09:13.458513Z","caller":"traceutil/trace.go:171","msg":"trace[207601283] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:11739; }","duration":"223.992495ms","start":"2024-07-02T14:09:13.234514Z","end":"2024-07-02T14:09:13.458507Z","steps":["trace[207601283] 'agreement among raft nodes before linearized reading'  (duration: 223.948184ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-02T14:09:16.31233Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"252.661389ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-07-02T14:09:16.31238Z","caller":"traceutil/trace.go:171","msg":"trace[288370814] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11742; }","duration":"252.745782ms","start":"2024-07-02T14:09:16.059622Z","end":"2024-07-02T14:09:16.312368Z","steps":["trace[288370814] 'range keys from in-memory index tree'  (duration: 252.565095ms)"],"step_count":1}
{"level":"info","ts":"2024-07-02T14:10:29.54333Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11574}
{"level":"info","ts":"2024-07-02T14:10:29.606397Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11574,"took":"58.812237ms","hash":361657521,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1232896,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-07-02T14:10:29.60819Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":361657521,"revision":11574,"compact-revision":11408}
{"level":"info","ts":"2024-07-02T14:15:29.546938Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11802}
{"level":"info","ts":"2024-07-02T14:15:29.559203Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11802,"took":"11.051938ms","hash":3790403789,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1368064,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:15:29.559248Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3790403789,"revision":11802,"compact-revision":11574}
{"level":"info","ts":"2024-07-02T14:20:29.547492Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12042}
{"level":"info","ts":"2024-07-02T14:20:29.550195Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12042,"took":"2.43521ms","hash":481264849,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1404928,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:20:29.550227Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":481264849,"revision":12042,"compact-revision":11802}
{"level":"info","ts":"2024-07-02T14:25:29.546989Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12282}
{"level":"info","ts":"2024-07-02T14:25:29.548889Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12282,"took":"1.739207ms","hash":2323722695,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1376256,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:25:29.548921Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2323722695,"revision":12282,"compact-revision":12042}
{"level":"info","ts":"2024-07-02T14:30:29.54605Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12523}
{"level":"info","ts":"2024-07-02T14:30:29.548233Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12523,"took":"1.907645ms","hash":991771539,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:30:29.54826Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":991771539,"revision":12523,"compact-revision":12282}
{"level":"info","ts":"2024-07-02T14:35:29.545177Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12761}
{"level":"info","ts":"2024-07-02T14:35:29.547743Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12761,"took":"2.412123ms","hash":1998987712,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:35:29.547774Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1998987712,"revision":12761,"compact-revision":12523}
{"level":"info","ts":"2024-07-02T14:40:29.545331Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13002}
{"level":"info","ts":"2024-07-02T14:40:29.547928Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13002,"took":"2.461485ms","hash":2105691052,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:40:29.547959Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2105691052,"revision":13002,"compact-revision":12761}
{"level":"info","ts":"2024-07-02T14:45:14.999541Z","caller":"traceutil/trace.go:171","msg":"trace[470613776] transaction","detail":"{read_only:false; response_revision:13471; number_of_response:1; }","duration":"345.348194ms","start":"2024-07-02T14:45:14.654173Z","end":"2024-07-02T14:45:14.999521Z","steps":["trace[470613776] 'process raft request'  (duration: 345.236189ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-02T14:45:14.999693Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-02T14:45:14.654159Z","time spent":"345.435787ms","remote":"127.0.0.1:58110","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13469 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-07-02T14:45:29.536621Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13242}
{"level":"info","ts":"2024-07-02T14:45:29.538755Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13242,"took":"2.005385ms","hash":485419868,"current-db-size-bytes":1970176,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-02T14:45:29.538789Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":485419868,"revision":13242,"compact-revision":13002}
{"level":"info","ts":"2024-07-02T14:48:18.404747Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-07-02T14:48:18.415427Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-07-02T14:48:18.417328Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-02T14:48:18.42124Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-02T14:48:18.421328Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-02T14:48:18.421411Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2024/07/02 14:48:18 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"info","ts":"2024-07-02T14:48:18.5111Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2024-07-02T14:48:18.511175Z","caller":"etcdserver/server.go:1165","msg":"failed to revoke lease","lease-id":"70cc9073682d2992","error":"etcdserver: request cancelled"}
{"level":"info","ts":"2024-07-02T14:48:18.615513Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-07-02T14:48:18.615718Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-07-02T14:48:18.615746Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 15:00:05 up  1:49,  0 users,  load average: 0.29, 0.30, 0.33
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [3a5e87980c9f] <==
I0702 14:48:18.515371       1 controller.go:176] quota evaluator worker shutdown
I0702 14:48:18.515393       1 controller.go:176] quota evaluator worker shutdown
I0702 14:48:18.515398       1 controller.go:176] quota evaluator worker shutdown
I0702 14:48:18.515402       1 controller.go:176] quota evaluator worker shutdown
W0702 14:48:19.446412       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456179       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456200       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456223       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456244       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456247       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456255       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456279       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456295       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456307       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456319       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456279       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.456343       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458749       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458783       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458785       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458810       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458819       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458842       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458859       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458886       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.458957       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459051       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459089       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459102       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459121       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459148       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459163       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459188       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459208       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459226       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459251       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459321       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459331       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459344       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459357       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459380       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.459423       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461720       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461728       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461755       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461757       1 logging.go:59] [core] [Channel #16 SubChannel #17] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461776       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461790       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461805       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461812       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461777       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461835       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461848       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.462092       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.462122       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.461836       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.462098       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.462111       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.462158       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0702 14:48:19.466746       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [61bd8084c64a] <==
I0703 14:31:32.762798       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0703 14:31:32.762801       1 cache.go:39] Caches are synced for autoregister controller
I0703 14:31:32.774330       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0703 14:31:32.810556       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0703 14:31:32.822202       1 shared_informer.go:320] Caches are synced for node_authorizer
E0703 14:31:32.855215       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 2e7ebf45-a2b3-4c20-b677-90674fc95f35, UID in object meta: "
I0703 14:31:33.660974       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0703 14:31:45.345254       1 controller.go:615] quota admission added evaluator for: endpoints
I0703 14:31:46.074173       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0703 14:37:41.609664       1 alloc.go:330] "allocated clusterIPs" service="default/api-service" clusterIPs={"IPv4":"10.106.51.157"}
I0703 14:40:45.253495       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0703 14:40:45.267997       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0703 14:45:03.610734       1 trace.go:236] Trace[1857559383]: "Update" accept:application/json, */*,audit-id:437b4b0a-78ce-4de0-b497-b46735ae0e45,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (03-Jul-2024 14:45:03.026) (total time: 580ms):
Trace[1857559383]: ["GuaranteedUpdate etcd3" audit-id:437b4b0a-78ce-4de0-b497-b46735ae0e45,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 580ms (14:45:03.026)
Trace[1857559383]:  ---"Txn call completed" 579ms (14:45:03.606)]
Trace[1857559383]: [580.102877ms] [580.102877ms] END
I0703 14:45:04.665963       1 trace.go:236] Trace[669921772]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (03-Jul-2024 14:45:03.814) (total time: 851ms):
Trace[669921772]: ---"initial value restored" 846ms (14:45:04.660)
Trace[669921772]: [851.769729ms] [851.769729ms] END
I0703 14:45:06.415037       1 trace.go:236] Trace[319683551]: "Get" accept:application/json, */*,audit-id:e387b0df-6475-48e9-9d59-36490cb8af02,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (03-Jul-2024 14:45:05.612) (total time: 802ms):
Trace[319683551]: ---"About to write a response" 802ms (14:45:06.414)
Trace[319683551]: [802.196395ms] [802.196395ms] END
I0703 14:45:07.333850       1 trace.go:236] Trace[663428327]: "Update" accept:application/json, */*,audit-id:9d196017-0ece-4717-ba24-51bad2fc5b02,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (03-Jul-2024 14:45:06.416) (total time: 917ms):
Trace[663428327]: ["GuaranteedUpdate etcd3" audit-id:9d196017-0ece-4717-ba24-51bad2fc5b02,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 917ms (14:45:06.416)
Trace[663428327]:  ---"Txn call completed" 916ms (14:45:07.333)]
Trace[663428327]: [917.259606ms] [917.259606ms] END
I0703 14:49:45.329498       1 alloc.go:330] "allocated clusterIPs" service="default/api-service-v1" clusterIPs={"IPv4":"10.98.4.214"}
I0703 14:54:34.442930       1 trace.go:236] Trace[1107521643]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (03-Jul-2024 14:54:33.830) (total time: 612ms):
Trace[1107521643]: ---"Transaction prepared" 118ms (14:54:33.949)
Trace[1107521643]: ---"Txn call completed" 493ms (14:54:34.442)
Trace[1107521643]: [612.135552ms] [612.135552ms] END
I0703 14:54:39.448681       1 trace.go:236] Trace[1140330750]: "Update" accept:application/json, */*,audit-id:dd92d0cf-72aa-4303-a879-f5edcc60b4a2,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (03-Jul-2024 14:54:38.798) (total time: 650ms):
Trace[1140330750]: ["GuaranteedUpdate etcd3" audit-id:dd92d0cf-72aa-4303-a879-f5edcc60b4a2,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 650ms (14:54:38.798)
Trace[1140330750]:  ---"Txn call completed" 649ms (14:54:39.448)]
Trace[1140330750]: [650.301534ms] [650.301534ms] END
I0703 14:54:40.378590       1 trace.go:236] Trace[1155463189]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:91f269c9-f3c5-46b6-8769-3b825da5d9b5,client:192.168.49.2,api-group:,api-version:v1,name:mongo-pvc,subresource:,namespace:default,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:resource,url:/api/v1/namespaces/default/persistentvolumeclaims/mongo-pvc,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:GET (03-Jul-2024 14:54:39.627) (total time: 751ms):
Trace[1155463189]: ---"About to write a response" 751ms (14:54:40.378)
Trace[1155463189]: [751.309819ms] [751.309819ms] END
I0703 14:54:41.993993       1 trace.go:236] Trace[235676970]: "Update" accept:application/json, */*,audit-id:09827bda-2f2c-4b6c-873d-b2ce34e2bc49,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (03-Jul-2024 14:54:41.452) (total time: 541ms):
Trace[235676970]: ["GuaranteedUpdate etcd3" audit-id:09827bda-2f2c-4b6c-873d-b2ce34e2bc49,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 541ms (14:54:41.452)
Trace[235676970]:  ---"Txn call completed" 540ms (14:54:41.993)]
Trace[235676970]: [541.257598ms] [541.257598ms] END
I0703 14:54:43.979978       1 trace.go:236] Trace[1432249992]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f43aa65c-6495-4a5f-b804-de6df859166c,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (03-Jul-2024 14:54:42.911) (total time: 1068ms):
Trace[1432249992]: ["GuaranteedUpdate etcd3" audit-id:f43aa65c-6495-4a5f-b804-de6df859166c,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1068ms (14:54:42.911)
Trace[1432249992]:  ---"Txn call completed" 1068ms (14:54:43.979)]
Trace[1432249992]: [1.068713906s] [1.068713906s] END
I0703 14:54:44.779587       1 trace.go:236] Trace[1643767930]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (03-Jul-2024 14:54:43.831) (total time: 947ms):
Trace[1643767930]: ---"initial value restored" 147ms (14:54:43.979)
Trace[1643767930]: ---"Transaction prepared" 409ms (14:54:44.389)
Trace[1643767930]: ---"Txn call completed" 390ms (14:54:44.779)
Trace[1643767930]: [947.66416ms] [947.66416ms] END
I0703 14:54:55.136349       1 trace.go:236] Trace[324786298]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4b965e55-1c40-4034-9075-515e72d4d145,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (03-Jul-2024 14:54:54.314) (total time: 822ms):
Trace[324786298]: ["GuaranteedUpdate etcd3" audit-id:4b965e55-1c40-4034-9075-515e72d4d145,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 822ms (14:54:54.314)
Trace[324786298]:  ---"Txn call completed" 821ms (14:54:55.136)]
Trace[324786298]: [822.11404ms] [822.11404ms] END
I0703 14:54:55.205371       1 trace.go:236] Trace[1071532610]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (03-Jul-2024 14:54:53.832) (total time: 1372ms):
Trace[1071532610]: ---"initial value restored" 417ms (14:54:54.250)
Trace[1071532610]: ---"Transaction prepared" 885ms (14:54:55.136)
Trace[1071532610]: ---"Txn call completed" 69ms (14:54:55.205)
Trace[1071532610]: [1.372465279s] [1.372465279s] END


==> kube-controller-manager [a4e8519f1aa0] <==
I0702 12:25:40.527093       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0702 12:25:40.528284       1 shared_informer.go:320] Caches are synced for taint
I0702 12:25:40.528342       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0702 12:25:40.528399       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0702 12:25:40.529004       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0702 12:25:40.537684       1 shared_informer.go:320] Caches are synced for HPA
I0702 12:25:40.541844       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0702 12:25:40.541892       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0702 12:25:40.541855       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0702 12:25:40.701957       1 shared_informer.go:320] Caches are synced for job
I0702 12:25:40.704105       1 shared_informer.go:320] Caches are synced for TTL after finished
I0702 12:25:40.705216       1 shared_informer.go:320] Caches are synced for resource quota
I0702 12:25:40.724341       1 shared_informer.go:320] Caches are synced for cronjob
I0702 12:25:40.732762       1 shared_informer.go:320] Caches are synced for resource quota
I0702 12:25:41.155300       1 shared_informer.go:320] Caches are synced for garbage collector
I0702 12:25:41.223720       1 shared_informer.go:320] Caches are synced for garbage collector
I0702 12:25:41.223755       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0702 12:47:02.774649       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="52.845594ms"
I0702 12:47:02.782763       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="8.044254ms"
I0702 12:47:02.782949       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="25.081µs"
I0702 12:47:02.795287       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="25.341µs"
I0702 12:47:02.795522       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="22.971µs"
I0702 12:47:02.864015       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="40.592µs"
I0702 12:47:02.872501       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="46.632µs"
I0702 12:48:27.396065       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="1.888643ms"
I0702 12:48:27.405884       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="4.131219ms"
I0702 12:48:29.384533       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="19.440843ms"
I0702 12:48:29.385192       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="45.162µs"
I0702 12:48:30.446549       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="6.67733ms"
I0702 12:48:30.446625       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="36.721µs"
I0702 12:48:31.448608       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="103.835µs"
I0702 12:48:31.459893       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="48.742µs"
I0702 12:48:32.643822       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="58.863µs"
I0702 12:48:46.534890       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="54.323µs"
I0702 12:48:47.547141       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="39.772µs"
I0702 12:48:48.566598       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="87.764µs"
I0702 12:48:53.129567       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="43.902µs"
I0702 12:49:09.716127       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="40.012µs"
I0702 12:49:13.130624       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="35.712µs"
I0702 12:49:19.787051       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="52.462µs"
I0702 12:49:23.133857       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="39.372µs"
I0702 12:49:54.993502       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="43.372µs"
I0702 12:50:03.129869       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="45.943µs"
I0702 12:50:05.084459       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="58.743µs"
I0702 12:50:13.129616       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="40.482µs"
I0702 12:51:21.504535       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="7.418765ms"
I0702 12:51:21.505202       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="42.672µs"
I0702 12:51:22.509107       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="7.294311ms"
I0702 12:51:22.509205       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="39.991µs"
I0702 12:51:23.511454       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="52.613µs"
I0702 12:51:28.549290       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="624.589µs"
I0702 12:51:33.130006       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="42.342µs"
I0702 12:54:06.536270       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="25.282479ms"
I0702 12:54:06.536404       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="61.623µs"
I0702 12:54:07.511991       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="5.616401ms"
I0702 12:54:07.512837       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="43.902µs"
I0702 12:54:13.125578       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="62.403µs"
I0702 12:54:18.582515       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="38.462µs"
I0702 12:54:23.127671       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="55.222µs"
I0702 12:58:05.616093       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-api-deployment-7bfd8bf8f9" duration="6.45µs"


==> kube-controller-manager [dbb49555632a] <==
I0703 14:31:45.346874       1 shared_informer.go:320] Caches are synced for job
I0703 14:31:45.346902       1 shared_informer.go:320] Caches are synced for service account
I0703 14:31:45.347170       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="641.546µs"
I0703 14:31:45.349169       1 shared_informer.go:320] Caches are synced for deployment
I0703 14:31:45.350390       1 shared_informer.go:320] Caches are synced for TTL after finished
I0703 14:31:45.351053       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0703 14:31:45.351513       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0703 14:31:45.351617       1 shared_informer.go:320] Caches are synced for crt configmap
I0703 14:31:45.352757       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0703 14:31:45.352775       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0703 14:31:45.365314       1 shared_informer.go:320] Caches are synced for expand
I0703 14:31:45.366432       1 shared_informer.go:320] Caches are synced for namespace
I0703 14:31:45.369890       1 shared_informer.go:320] Caches are synced for HPA
I0703 14:31:45.378078       1 shared_informer.go:320] Caches are synced for disruption
I0703 14:31:45.380368       1 shared_informer.go:320] Caches are synced for PV protection
I0703 14:31:45.382665       1 shared_informer.go:320] Caches are synced for cronjob
I0703 14:31:45.384928       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0703 14:31:45.445188       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0703 14:31:45.477365       1 shared_informer.go:320] Caches are synced for resource quota
I0703 14:31:45.522168       1 shared_informer.go:320] Caches are synced for resource quota
I0703 14:31:45.573248       1 shared_informer.go:320] Caches are synced for stateful set
I0703 14:31:45.978272       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0703 14:31:45.978858       1 shared_informer.go:320] Caches are synced for garbage collector
I0703 14:31:45.980973       1 shared_informer.go:320] Caches are synced for daemon sets
I0703 14:31:45.982593       1 shared_informer.go:320] Caches are synced for taint
I0703 14:31:45.982655       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0703 14:31:45.982741       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0703 14:31:45.983087       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0703 14:31:46.023646       1 shared_informer.go:320] Caches are synced for node
I0703 14:31:46.023694       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0703 14:31:46.023718       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0703 14:31:46.023747       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0703 14:31:46.023765       1 shared_informer.go:320] Caches are synced for cidrallocator
I0703 14:31:46.040881       1 shared_informer.go:320] Caches are synced for GC
I0703 14:31:46.043151       1 shared_informer.go:320] Caches are synced for garbage collector
I0703 14:31:46.043165       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0703 14:31:46.047552       1 shared_informer.go:320] Caches are synced for TTL
I0703 14:31:46.055158       1 shared_informer.go:320] Caches are synced for persistent volume
I0703 14:31:46.060332       1 shared_informer.go:320] Caches are synced for attach detach
I0703 14:31:46.060733       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0703 14:31:46.068622       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0703 14:40:45.328690       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="49.986171ms"
I0703 14:40:45.337051       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="8.316597ms"
I0703 14:40:45.337343       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="49.693µs"
I0703 14:40:45.349998       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="94.336µs"
I0703 14:40:45.369213       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="38.303µs"
I0703 14:41:07.306053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="3.04957ms"
I0703 14:41:25.600868       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="65.964µs"
I0703 14:41:53.597751       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="34.002µs"
I0703 14:42:05.619307       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="65.034µs"
I0703 14:42:46.596043       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="39.193µs"
I0703 14:42:58.594522       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="40.362µs"
I0703 14:44:09.632605       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="58.863µs"
I0703 14:44:21.592537       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="92.066µs"
I0703 14:46:53.587704       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="60.944µs"
I0703 14:47:07.588156       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="80.485µs"
I0703 14:52:07.579987       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="165.719µs"
I0703 14:52:19.580188       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="42.532µs"
I0703 14:57:19.572399       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="69.084µs"
I0703 14:57:32.572723       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/manage-app-v1-65dcb4445b" duration="67.353µs"


==> kube-proxy [5ba6afcb3788] <==
I0702 12:25:29.581268       1 server_linux.go:69] "Using iptables proxy"
I0702 12:25:29.646900       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0702 12:25:29.750867       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0702 12:25:29.750937       1 server_linux.go:165] "Using iptables Proxier"
I0702 12:25:29.752090       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0702 12:25:29.752123       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0702 12:25:29.756935       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0702 12:25:29.760049       1 server.go:872] "Version info" version="v1.30.0"
I0702 12:25:29.760078       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0702 12:25:29.768559       1 config.go:192] "Starting service config controller"
I0702 12:25:29.770058       1 shared_informer.go:313] Waiting for caches to sync for service config
I0702 12:25:29.770109       1 config.go:101] "Starting endpoint slice config controller"
I0702 12:25:29.770116       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0702 12:25:29.771099       1 config.go:319] "Starting node config controller"
I0702 12:25:29.771128       1 shared_informer.go:313] Waiting for caches to sync for node config
I0702 12:25:29.871021       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0702 12:25:29.871045       1 shared_informer.go:320] Caches are synced for service config
I0702 12:25:29.871267       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [9ad9beb122cd] <==
I0703 14:31:35.232680       1 server_linux.go:69] "Using iptables proxy"
I0703 14:31:35.267198       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0703 14:31:35.315282       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0703 14:31:35.315349       1 server_linux.go:165] "Using iptables Proxier"
I0703 14:31:35.316666       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0703 14:31:35.316692       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0703 14:31:35.317490       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0703 14:31:35.318535       1 server.go:872] "Version info" version="v1.30.0"
I0703 14:31:35.318565       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0703 14:31:35.322014       1 config.go:192] "Starting service config controller"
I0703 14:31:35.322040       1 shared_informer.go:313] Waiting for caches to sync for service config
I0703 14:31:35.322839       1 config.go:101] "Starting endpoint slice config controller"
I0703 14:31:35.323011       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0703 14:31:35.323044       1 config.go:319] "Starting node config controller"
I0703 14:31:35.323048       1 shared_informer.go:313] Waiting for caches to sync for node config
I0703 14:31:35.422147       1 shared_informer.go:320] Caches are synced for service config
I0703 14:31:35.423260       1 shared_informer.go:320] Caches are synced for node config
I0703 14:31:35.423273       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [919a141190dd] <==
I0702 12:25:25.488633       1 serving.go:380] Generated self-signed cert in-memory
W0702 12:25:28.218326       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0702 12:25:28.218361       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0702 12:25:28.218370       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0702 12:25:28.218375       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0702 12:25:28.237508       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0702 12:25:28.237554       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0702 12:25:28.242818       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0702 12:25:28.242840       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0702 12:25:28.242846       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0702 12:25:28.242896       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0702 12:25:28.343315       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0702 14:48:18.381783       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0702 14:48:18.395614       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0702 14:48:18.396976       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0702 14:48:18.411261       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [e9402540f571] <==
I0703 14:31:31.659797       1 serving.go:380] Generated self-signed cert in-memory
W0703 14:31:32.726064       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0703 14:31:32.726160       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0703 14:31:32.726193       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0703 14:31:32.726272       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0703 14:31:32.752257       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0703 14:31:32.752277       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0703 14:31:32.757067       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0703 14:31:32.757151       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0703 14:31:32.757159       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0703 14:31:32.757183       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0703 14:31:32.857841       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jul 03 14:48:20 minikube kubelet[1426]: E0703 14:48:20.578590    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:48:35 minikube kubelet[1426]: E0703 14:48:35.577821    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:48:50 minikube kubelet[1426]: E0703 14:48:50.579033    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:49:04 minikube kubelet[1426]: E0703 14:49:04.577072    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:49:19 minikube kubelet[1426]: E0703 14:49:19.577094    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:49:31 minikube kubelet[1426]: E0703 14:49:31.576388    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:49:45 minikube kubelet[1426]: E0703 14:49:45.576025    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:49:57 minikube kubelet[1426]: E0703 14:49:57.575965    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:50:09 minikube kubelet[1426]: E0703 14:50:09.575918    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:50:20 minikube kubelet[1426]: E0703 14:50:20.576351    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:50:34 minikube kubelet[1426]: E0703 14:50:34.574889    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:50:46 minikube kubelet[1426]: E0703 14:50:46.574981    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:50:58 minikube kubelet[1426]: E0703 14:50:58.574667    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:51:12 minikube kubelet[1426]: E0703 14:51:12.574610    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:51:24 minikube kubelet[1426]: E0703 14:51:24.574099    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:51:38 minikube kubelet[1426]: E0703 14:51:38.573465    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:51:54 minikube kubelet[1426]: E0703 14:51:54.825882    1426 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown" image="lautaro124/manage_app:lastest"
Jul 03 14:51:54 minikube kubelet[1426]: E0703 14:51:54.825947    1426 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown" image="lautaro124/manage_app:lastest"
Jul 03 14:51:54 minikube kubelet[1426]: E0703 14:51:54.826066    1426 kuberuntime_manager.go:1256] container &Container{Name:manage-app-v1,Image:lautaro124/manage_app:lastest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mongo-persistent-storage,ReadOnly:false,MountPath:/data/db,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-5l6wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 3000 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:2,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod manage-app-v1-65dcb4445b-xh6z9_default(4302d968-1445-4247-9804-19dbf7a037a5): ErrImagePull: Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown
Jul 03 14:51:54 minikube kubelet[1426]: E0703 14:51:54.826089    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ErrImagePull: \"Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:52:07 minikube kubelet[1426]: E0703 14:52:07.572850    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:52:19 minikube kubelet[1426]: E0703 14:52:19.572818    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:52:33 minikube kubelet[1426]: E0703 14:52:33.572176    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:52:48 minikube kubelet[1426]: E0703 14:52:48.572721    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:53:03 minikube kubelet[1426]: E0703 14:53:03.571033    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:53:14 minikube kubelet[1426]: E0703 14:53:14.571741    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:53:27 minikube kubelet[1426]: E0703 14:53:27.570406    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:53:38 minikube kubelet[1426]: E0703 14:53:38.571407    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:53:50 minikube kubelet[1426]: E0703 14:53:50.570522    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:54:01 minikube kubelet[1426]: E0703 14:54:01.570281    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:54:14 minikube kubelet[1426]: E0703 14:54:14.569510    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:54:25 minikube kubelet[1426]: E0703 14:54:25.569574    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:54:39 minikube kubelet[1426]: E0703 14:54:39.568970    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:54:51 minikube kubelet[1426]: E0703 14:54:51.569026    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:55:03 minikube kubelet[1426]: E0703 14:55:03.568711    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:55:17 minikube kubelet[1426]: E0703 14:55:17.568524    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:55:30 minikube kubelet[1426]: E0703 14:55:30.568283    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:55:44 minikube kubelet[1426]: E0703 14:55:44.567027    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:55:55 minikube kubelet[1426]: E0703 14:55:55.567273    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:56:06 minikube kubelet[1426]: E0703 14:56:06.567589    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:56:19 minikube kubelet[1426]: E0703 14:56:19.566909    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:56:34 minikube kubelet[1426]: E0703 14:56:34.566592    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:56:47 minikube kubelet[1426]: E0703 14:56:47.566335    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:57:04 minikube kubelet[1426]: E0703 14:57:04.905856    1426 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown" image="lautaro124/manage_app:lastest"
Jul 03 14:57:04 minikube kubelet[1426]: E0703 14:57:04.905933    1426 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown" image="lautaro124/manage_app:lastest"
Jul 03 14:57:04 minikube kubelet[1426]: E0703 14:57:04.906046    1426 kuberuntime_manager.go:1256] container &Container{Name:manage-app-v1,Image:lautaro124/manage_app:lastest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mongo-persistent-storage,ReadOnly:false,MountPath:/data/db,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-5l6wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 3000 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:2,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod manage-app-v1-65dcb4445b-xh6z9_default(4302d968-1445-4247-9804-19dbf7a037a5): ErrImagePull: Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown
Jul 03 14:57:04 minikube kubelet[1426]: E0703 14:57:04.906069    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ErrImagePull: \"Error response from daemon: manifest for lautaro124/manage_app:lastest not found: manifest unknown: manifest unknown\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:57:19 minikube kubelet[1426]: E0703 14:57:19.565667    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:57:32 minikube kubelet[1426]: E0703 14:57:32.564580    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:57:43 minikube kubelet[1426]: E0703 14:57:43.564551    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:57:58 minikube kubelet[1426]: E0703 14:57:58.563902    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:58:12 minikube kubelet[1426]: E0703 14:58:12.563740    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:58:24 minikube kubelet[1426]: E0703 14:58:24.564411    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:58:38 minikube kubelet[1426]: E0703 14:58:38.563831    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:58:49 minikube kubelet[1426]: E0703 14:58:49.563152    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:59:03 minikube kubelet[1426]: E0703 14:59:03.562769    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:59:16 minikube kubelet[1426]: E0703 14:59:16.562339    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:59:29 minikube kubelet[1426]: E0703 14:59:29.562450    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:59:44 minikube kubelet[1426]: E0703 14:59:44.566996    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"
Jul 03 14:59:55 minikube kubelet[1426]: E0703 14:59:55.562007    1426 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"manage-app-v1\" with ImagePullBackOff: \"Back-off pulling image \\\"lautaro124/manage_app:lastest\\\"\"" pod="default/manage-app-v1-65dcb4445b-xh6z9" podUID="4302d968-1445-4247-9804-19dbf7a037a5"


==> storage-provisioner [52da41a492d7] <==
I0703 14:31:50.731471       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0703 14:31:50.739929       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0703 14:31:50.740730       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0703 14:32:08.136546       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0703 14:32:08.136617       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"3cde2dff-110e-4461-9b3d-925ceac065c1", APIVersion:"v1", ResourceVersion:"13719", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_de24de53-99a2-4bff-9570-e4599f1db952 became leader
I0703 14:32:08.136741       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_de24de53-99a2-4bff-9570-e4599f1db952!
I0703 14:32:08.238815       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_de24de53-99a2-4bff-9570-e4599f1db952!
I0703 14:37:41.649384       1 controller.go:1332] provision "default/mongo-pvc" class "standard": started
I0703 14:37:41.652359       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc", UID:"cec089b0-81b1-4851-8f1d-72b54afad7f9", APIVersion:"v1", ResourceVersion:"13995", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-pvc"
I0703 14:37:41.651225       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    ac263db5-f1aa-42c4-aa91-f7b33e60c3fe 307 0 2024-07-02 00:49:43 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-07-02 00:49:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-cec089b0-81b1-4851-8f1d-72b54afad7f9 &PersistentVolumeClaim{ObjectMeta:{mongo-pvc  default  cec089b0-81b1-4851-8f1d-72b54afad7f9 13995 0 2024-07-03 14:37:41 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"mongo-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"10Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-07-03 14:37:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2024-07-03 14:37:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-pvc
I0703 14:37:41.660194       1 controller.go:1439] provision "default/mongo-pvc" class "standard": volume "pvc-cec089b0-81b1-4851-8f1d-72b54afad7f9" provisioned
I0703 14:37:41.660226       1 controller.go:1456] provision "default/mongo-pvc" class "standard": succeeded
I0703 14:37:41.660231       1 volume_store.go:212] Trying to save persistentvolume "pvc-cec089b0-81b1-4851-8f1d-72b54afad7f9"
I0703 14:37:41.665174       1 volume_store.go:219] persistentvolume "pvc-cec089b0-81b1-4851-8f1d-72b54afad7f9" saved
I0703 14:37:41.665376       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc", UID:"cec089b0-81b1-4851-8f1d-72b54afad7f9", APIVersion:"v1", ResourceVersion:"13995", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-cec089b0-81b1-4851-8f1d-72b54afad7f9


==> storage-provisioner [5fa3962cd307] <==
I0703 14:31:34.881728       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0703 14:31:34.933005       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

